{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/tf/pollock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "from importlib import reload\n",
    "import time\n",
    "\n",
    "import anndata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pollock\n",
    "from pollock import PollockDataset, PollockModel, load_from_directory\n",
    "# import pollock.models.analysis as pollock_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 9940628303412410795,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 3818076523845842837\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 7657496466880006617\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:XLA_GPU:1\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 6821635573982979800\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:XLA_GPU:2\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 3901834504750236522\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:XLA_GPU:3\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 13431627675899494463\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 23875656090\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 5161054069016094911\n",
       " physical_device_desc: \"device: 0, name: TITAN RTX, pci bus id: 0000:1a:00.0, compute capability: 7.5\",\n",
       " name: \"/device:GPU:1\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 23875656090\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 15783517144675939903\n",
       " physical_device_desc: \"device: 1, name: TITAN RTX, pci bus id: 0000:3d:00.0, compute capability: 7.5\",\n",
       " name: \"/device:GPU:2\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 23875656090\n",
       " locality {\n",
       "   bus_id: 2\n",
       "   numa_node: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 684178082288832789\n",
       " physical_device_desc: \"device: 2, name: TITAN RTX, pci bus id: 0000:89:00.0, compute capability: 7.5\",\n",
       " name: \"/device:GPU:3\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 23875656090\n",
       " locality {\n",
       "   bus_id: 2\n",
       "   numa_node: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 14244643532550401317\n",
       " physical_device_desc: \"device: 3, name: TITAN RTX, pci bus id: 0000:b2:00.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/estorrs/data/single_cell_classification'\n",
    "MODEL_DIR = '/home/estorrs/pollock/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/data/single_cell_classification'\n",
    "MODEL_DIR = '/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'br'\n",
    "\n",
    "expression_fp = os.path.join(DATA_DIR, 'tumor', 'BR', 'raw', 'houxiang_brca',\n",
    "                            'breast_counts_matrix.tsv')\n",
    "label_fp = os.path.join(DATA_DIR, 'tumor', 'BR', 'raw', 'houxiang_brca',\n",
    "                            'breast_metadata.tsv')\n",
    "\n",
    "training_image_dir = os.path.join(MODEL_DIR, 'scratch', run_name)\n",
    "model_save_dir = os.path.join(MODEL_DIR, run_name)\n",
    "\n",
    "sample_column = 'Genes'\n",
    "sep='\\t'\n",
    "cell_type_key = 'cell_type'\n",
    "\n",
    "n_per_cell_type = 5000\n",
    "epochs = 5\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_df = pd.read_hdf(expression_fp.replace('.tsv', '.h5'), 'df')\n",
    "expression_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.read_csv(\n",
    "    label_fp,\n",
    "    sep=sep\n",
    "    )\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = label_df.set_index('cell_id')\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = label_df.loc[expression_df.index]\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = anndata.AnnData(X=expression_df.values, obs=label_df)\n",
    "adata.obs.index = expression_df.index\n",
    "adata.var.index = expression_df.columns\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(adata.obs[cell_type_key])\n",
    "counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get rid of unknowns\n",
    "adata = adata[adata.obs[cell_type_key]!='Unknown']\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.filter_cells(adata, min_genes=200)\n",
    "sc.pp.filter_genes(adata, min_cells=3)\n",
    "\n",
    "# mito_genes = adata.var_names.str.startswith('MT-')\n",
    "# # for each cell compute fraction of counts in mito genes vs. all genes\n",
    "# # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing)\n",
    "# adata.obs['percent_mito'] = np.sum(\n",
    "#     adata[:, mito_genes].X, axis=1) / np.sum(adata.X, axis=1)\n",
    "# # add the total counts per cell as observations-annotation to adata\n",
    "# adata.obs['n_counts'] = adata.X.sum(axis=1)\n",
    "\n",
    "# sc.pl.scatter(adata, x='n_counts', y='percent_mito')\n",
    "# sc.pl.scatter(adata, x='n_counts', y='n_genes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata = adata[adata.obs.n_genes < 6000, :]\n",
    "# adata = adata[adata.obs.percent_mito < 0.05, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "adata.raw = adata\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.pp.highly_variable_genes(adata, min_mean=.0125, max_mean=10., min_disp=0.2)\n",
    "sc.pp.highly_variable_genes(adata, min_mean=None, max_mean=None, min_disp=0.2)\n",
    "\n",
    "sc.pl.highly_variable_genes(adata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(adata.var.highly_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata[:, adata.var.highly_variable]\n",
    "# sc.pp.scale(adata, max_value=2.)\n",
    "sc.pp.scale(adata, max_value=None)\n",
    "\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.pca(adata, svd_solver='arpack')\n",
    "sc.pp.neighbors(adata, n_neighbors=10, n_pcs=30)\n",
    "sc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "## filter smartly\n",
    "n = 1000\n",
    "\n",
    "cell_type_to_idxs = {}\n",
    "for cell_id, cell_type in zip(adata.obs.index, adata.obs[cell_type_key]):\n",
    "    if cell_type not in cell_type_to_idxs:\n",
    "        cell_type_to_idxs[cell_type] = [cell_id]\n",
    "    else:\n",
    "        cell_type_to_idxs[cell_type].append(cell_id)\n",
    "        \n",
    "def temp(ls):\n",
    "    if len(ls) > n:\n",
    "        return random.sample(ls, n)\n",
    "    return random.sample(ls, int(len(ls) * .8))\n",
    "\n",
    "cell_type_to_idxs = {k:temp(ls)\n",
    "                     for k, ls in cell_type_to_idxs.items()}\n",
    "\n",
    "train_idxs = np.asarray([x for ls in cell_type_to_idxs.values() for x in ls])\n",
    "train_idxs = np.arange(adata.shape[0])[np.isin(np.asarray(adata.obs.index), train_idxs)]\n",
    "val_idxs = np.delete(np.arange(adata.shape[0]), train_idxs)\n",
    "\n",
    "train_idxs.shape, val_idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_adata = adata[train_idxs, :].copy()\n",
    "val_adata = adata[val_idxs, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(val_adata.obs[cell_type_key]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(train_adata.obs[cell_type_key]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## binarize for first pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_adata.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.distplot(train_adata.X[:, train_adata.var.index=='KRT18'].flatten(), kde=False, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(train_adata.raw.X[:, train_adata.raw.var.index=='EPCAM'].flatten(), kde=False, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.copy(train_adata.X)\n",
    "X_val = np.copy(val_adata.X)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BUF = 10000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(TRAIN_BUF).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(X_val).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BVAE(tf.keras.Model):\n",
    "  def __init__(self, latent_dim, input_size):\n",
    "    super(BVAE, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    self.input_size = input_size\n",
    "    self.inference_net = tf.keras.Sequential(\n",
    "      [\n",
    "          tf.keras.layers.InputLayer(input_shape=(input_size,)),\n",
    "          tf.keras.layers.Dense(800, activation='relu'),\n",
    "          tf.keras.layers.Dropout(.2),\n",
    "          tf.keras.layers.Dense(800, activation='relu'),\n",
    "          tf.keras.layers.Dropout(.2),\n",
    "          tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    self.generative_net = tf.keras.Sequential(\n",
    "        [\n",
    "          tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "          tf.keras.layers.Dense(800, activation='relu'),\n",
    "          tf.keras.layers.Dropout(.2),\n",
    "          tf.keras.layers.Dense(800, activation='relu'),\n",
    "          tf.keras.layers.Dropout(.2),\n",
    "          tf.keras.layers.Dense(input_size),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "  @tf.function\n",
    "  def sample(self, eps=None):\n",
    "    if eps is None:\n",
    "      eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "    return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "  def encode(self, x):\n",
    "    mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
    "    return mean, logvar\n",
    "\n",
    "  def reparameterize(self, mean, logvar):\n",
    "    eps = tf.random.normal(shape=mean.shape)\n",
    "    return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "  def decode(self, z, apply_sigmoid=False):\n",
    "    logits = self.generative_net(z)\n",
    "    if apply_sigmoid:\n",
    "      probs = tf.sigmoid(logits)\n",
    "      return probs\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "  log2pi = tf.math.log(2. * np.pi)\n",
    "  return tf.reduce_sum(\n",
    "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "      axis=raxis)\n",
    "\n",
    "# @tf.function\n",
    "# def compute_loss(model, x):\n",
    "#   mean, logvar = model.encode(x)\n",
    "#   z = model.reparameterize(mean, logvar)\n",
    "#   x_logit = model.decode(z)\n",
    "\n",
    "# #   cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "# #   logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "\n",
    "#   logpx_z = tf.metrics.msle(x_logit, x)\n",
    "#   logpz = log_normal_pdf(z, 0., 0.)\n",
    "#   logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "\n",
    "#   return -tf.reduce_mean(logpx_z + ((logpz - logqz_x) * .0005))\n",
    "# #   return -tf.reduce_mean(logpx_z)\n",
    "\n",
    "\n",
    "\n",
    "#         def vae_loss(y_true, y_pred):\n",
    "#             return K.mean(recon_loss(y_true, y_pred) + self.alpha * kl_loss(y_true, y_pred))\n",
    "\n",
    "#         def kl_loss(y_true, y_pred):\n",
    "#             return 0.5 * K.sum(K.exp(self.log_var) + K.square(self.mu) - 1. - self.log_var, axis=1)\n",
    "\n",
    "#         def recon_loss(y_true, y_pred):\n",
    "#             return 0.5 * K.sum(K.square((y_true - y_pred)), axis=1)\n",
    "\n",
    "@tf.function\n",
    "def compute_loss(model, x, alpha=0.00005):\n",
    "  mean, logvar = model.encode(x)\n",
    "  z = model.reparameterize(mean, logvar)\n",
    "  x_logit = model.decode(z)\n",
    "\n",
    "  kl_loss = .5 * tf.reduce_sum(tf.exp(logvar) + tf.square(mean) - 1. - logvar, axis=1)\n",
    "  reconstruction_loss = .5 * tf.reduce_sum(tf.square((x - x_logit)), axis=1)\n",
    "\n",
    "  overall_loss = tf.reduce_mean(reconstruction_loss + alpha * kl_loss)\n",
    "  return overall_loss\n",
    "\n",
    "@tf.function\n",
    "def compute_apply_gradients(model, x, optimizer, alpha=.00005):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss = compute_loss(model, x, alpha=alpha)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "latent_dim = 100\n",
    "alpha = 0.1\n",
    "# num_examples_to_generate = 16\n",
    "\n",
    "# keeping the random vector constant for generation (prediction) so\n",
    "# it will be easier to see the improvement.\n",
    "# random_vector_for_generation = tf.random.normal(\n",
    "#     shape=[num_examples_to_generate, latent_dim])\n",
    "model = BVAE(latent_dim, X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_and_save_images(model, 0, random_vector_for_generation)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "  start_time = time.time()\n",
    "  for train_x in train_dataset:\n",
    "    compute_apply_gradients(model, train_x, optimizer, alpha=alpha)\n",
    "  end_time = time.time()\n",
    "\n",
    "  if epoch % 1 == 0:\n",
    "    loss = tf.keras.metrics.Mean()\n",
    "    for test_x in test_dataset:\n",
    "\n",
    "      \n",
    "      loss(compute_loss(model, test_x, alpha=alpha))\n",
    "    print(f'epoch: {epoch}, val loss: {loss.result()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, logvar = model.encode(train_adata.X)\n",
    "train_embeddings = model.reparameterize(mean, logvar).numpy()\n",
    "\n",
    "mean, logvar = model.encode(val_adata.X)\n",
    "val_embeddings = model.reparameterize(mean, logvar).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "encoder = OrdinalEncoder()\n",
    "y_train = encoder.fit_transform(np.asarray(train_adata.obs[cell_type_key]).reshape(-1, 1)).flatten()\n",
    "y_val = encoder.transform(np.asarray(val_adata.obs[cell_type_key]).reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf.fit(train_embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(train_embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(val_embeddings, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(val_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_mat = confusion_matrix(y_val, preds)\n",
    "c_mat = c_mat / np.sum(c_mat, axis=1).reshape(-1, 1)\n",
    "c_mat = (c_mat * 100).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 9))\n",
    "sns.heatmap(c_mat, xticklabels=encoder.categories_[0], yticklabels=encoder.categories_[0],\n",
    "           cmap='Blues',annot=True, fmt='d')\n",
    "plt.xlabel('predicted')\n",
    "plt.xlabel('true')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('br_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, logvar = model.encode(adata.X)\n",
    "cell_embeddings = model.reparameterize(mean, logvar).numpy()\n",
    "cell_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obsm['cell_embeddings'] = cell_embeddings\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_cell_types = [encoder.categories_[0][int(i)] for i in clf.predict(cell_embeddings)]\n",
    "adata.obs['predicted_cell_type'] = predicted_cell_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obsm['embedding_umap'] = umap.UMAP().fit_transform(adata.obsm['cell_embeddings'])\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata[val_idxs], color=['cell_type', 'predicted_cell_type', 'sample_id'], frameon=False, ncols=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['embedding_umap1'] = adata.obsm['embedding_umap'][:, 0]\n",
    "adata.obs['embedding_umap2'] = adata.obsm['embedding_umap'][:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.scatter(adata[val_idxs], x='embedding_umap1', y='embedding_umap2', color='cell_type',\n",
    "             frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.scatter(adata[val_idxs], x='embedding_umap1', y='embedding_umap2', color='predicted_cell_type',\n",
    "             frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.scatter(adata[val_idxs], x='embedding_umap1', y='embedding_umap2', color='sample_id',\n",
    "             frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
